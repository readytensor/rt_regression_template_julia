{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using DataFrames\n",
    "    using LazyJSON\n",
    "    using GLM \n",
    "    using MLJ \n",
    "    using MLJBase\n",
    "    using CSV\n",
    "    using Serialization\n",
    "    using MLJScientificTypes\n",
    "    using CategoricalArrays\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES \n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "OUTPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"outputs\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.jld2\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "PREDICTIONS_DIR = joinpath(OUTPUT_DIR, \"predictions\")\n",
    "PREDICTIONS_FILE = joinpath(PREDICTIONS_DIR, \"predictions.csv\")\n",
    "\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = filter(x -> occursin(\".csv\", x), readdir(TEST_DIR))[1]\n",
    "file_path = joinpath(TEST_DIR, file_name)\n",
    "df = DataFrame(CSV.File(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Note that when we work with testing data, we have to impute using the same values learned during training. This is to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_values = open(deserialize, IMPUTATION_FILE)\n",
    "for column in nullable_features\n",
    "    df[!, Symbol(column)] .= coalesce.(df[!, Symbol(column)], get(imputation_values, string(column), missing))\n",
    "end\n",
    "\n",
    "# Saving the id column in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "\n",
    "# Dropping the id and target from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "We encode the data using the same encoder that we saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_top_categories = open(deserialize, TOP_CATEGORIES)\n",
    "\n",
    "# Function to one-hot encode only the top 10 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = \"$(feature)_binary\"\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = \"$(feature)_$(cat)\"\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "one_hot_top_categories!(df, loaded_top_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "Using the model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = open(deserialize, PREDICTOR_FILE_PATH)\n",
    "\n",
    "# Make predictions\n",
    "predictions = GLM.predict(model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictions DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column_name = id_feature\n",
    "prediction_column_name = \"prediction\"\n",
    "# Create a DataFrame\n",
    "df = DataFrame()\n",
    "df[!, Symbol(id_column_name)] = ids\n",
    "df[!, Symbol(prediction_column_name)] = predictions\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "CSV.write(PREDICTIONS_FILE, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
